{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the necessary libraries\n!pip install -q diffusers transformers accelerate ftfy\n\nprint(\"Libraries installed successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import Subset\n\n# Define the transformation pipeline\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n\n# Download and load the FULL training data\nfull_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=preprocess)\n\n# --- SPEEDUP CHANGE: Use a subset of the data ---\n# We'll use only the first 10,000 images for faster training\nnum_images_for_subset = 10000\nsubset_indices = list(range(num_images_for_subset))\nsubset_dataset = Subset(full_dataset, subset_indices)\n\nprint(f\"Using a subset of {len(subset_dataset)} images instead of {len(full_dataset)}.\")\n\n# --- Use the subset_dataset for the dataloader ---\ndataloader = torch.utils.data.DataLoader(subset_dataset, batch_size=128, shuffle=True)\n\n# Visualization remains the same\ndataiter = iter(dataloader)\nimages, labels = next(dataiter)\nimg_grid = torchvision.utils.make_grid(images[:32])\nimg_grid = img_grid / 2 + 0.5\nnpimg = img_grid.numpy()\nplt.figure(figsize=(15, 7))\nplt.imshow(np.transpose(npimg, (1, 2, 0)))\nplt.axis('off')\nplt.title(\"Sample Images from CIFAR-10 Dataset Subset\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:28:57.74783Z","iopub.execute_input":"2025-08-06T12:28:57.748552Z","iopub.status.idle":"2025-08-06T12:29:11.589453Z","shell.execute_reply.started":"2025-08-06T12:28:57.748514Z","shell.execute_reply":"2025-08-06T12:29:11.588564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision\nfrom PIL import Image\nfrom dataclasses import dataclass\nfrom diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\nfrom torch.optim import AdamW\nfrom diffusers.optimization import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\nimport os\n\n# --- 1. Configuration ---\n@dataclass\nclass TrainingConfig:\n    image_size = 64\n    train_batch_size = 128\n    eval_batch_size = 16\n    num_epochs = 15  \n    learning_rate = 2e-4\n    lr_warmup_steps = 500\n    save_image_epochs = 2\n    output_dir = \"ddpm-cifar10-64-fast\"\n    seed = 0\n\nconfig = TrainingConfig()\n\n# --- 2. Dataloader ---\npreprocess = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((config.image_size, config.image_size)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.5], [0.5]),\n])\ndataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=preprocess)\n# Note: DataLoader batch_size is now the total size that will be split\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=2)\n\n# --- 3. Setup Device and Model ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = UNet2DModel(\n    sample_size=config.image_size, in_channels=3, out_channels=3, layers_per_block=2,\n    block_out_channels=(128, 128, 256, 256, 512, 512),\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n)\n\n# WRAP MODEL WITH nn.DataParallel\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel.to(device) # Move the model to the primary GPU\n\n# --- 4. Scheduler and Optimizer ---\nnoise_scheduler = DDPMScheduler(num_train_timesteps=1000)\noptimizer = AdamW(model.parameters(), lr=config.learning_rate)\nlr_scheduler = get_cosine_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=config.lr_warmup_steps,\n    num_training_steps=(len(dataloader) * config.num_epochs),\n)\n\n# --- 5. Training Loop ---\nfor epoch in range(config.num_epochs):\n    progress_bar = tqdm(total=len(dataloader))\n    progress_bar.set_description(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n\n    for step, batch in enumerate(dataloader):\n        clean_images = batch[0].to(device) # Move data to the device\n        noise = torch.randn(clean_images.shape).to(clean_images.device)\n        bs = clean_images.shape[0]\n\n        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device).long()\n        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n\n        # Predict the noise residual\n        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        progress_bar.update(1)\n        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n        progress_bar.set_postfix(**logs)\n\n    # --- 6. Generate and Save Images ---\n    if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n        # Access the original model through .module attribute\n        unwrapped_model = model.module if isinstance(model, nn.DataParallel) else model\n\n        pipeline = DDPMPipeline(unet=unwrapped_model, scheduler=noise_scheduler)\n        generator = torch.manual_seed(config.seed)\n        images = pipeline(generator=generator, batch_size=config.eval_batch_size, output_type=\"numpy\").images\n\n        print(f\"\\nGenerating {config.eval_batch_size} sample images at epoch {epoch+1}:\")\n        image_grid = torchvision.utils.make_grid(torch.from_numpy(images).permute(0, 3, 1, 2))\n        npimg = image_grid.numpy() / 2 + 0.5\n        plt.figure(figsize=(8, 8))\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n        plt.axis(\"off\")\n        plt.show()\n\nprint(\"\\nDiffusion model training complete!\")\n\n# --- 7. Save final model and store generated images ---\nfinal_unwrapped_model = model.module if isinstance(model, nn.DataParallel) else model\nfinal_pipeline = DDPMPipeline(unet=final_unwrapped_model, scheduler=noise_scheduler)\n\n# Create output directory if it doesn't exist\nif not os.path.exists(config.output_dir):\n    os.makedirs(config.output_dir)\n\nprint(f\"Saving final model pipeline to {config.output_dir}...\")\nfinal_pipeline.save_pretrained(config.output_dir)\n\n# Store the final generated images for later comparison\ndiffusion_images = final_pipeline(\n    generator=torch.manual_seed(config.seed),\n    batch_size=config.eval_batch_size,\n    output_type=\"numpy\"\n).images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:42:23.381426Z","iopub.execute_input":"2025-08-06T12:42:23.382264Z","iopub.status.idle":"2025-08-06T12:55:31.934496Z","shell.execute_reply.started":"2025-08-06T12:42:23.382229Z","shell.execute_reply":"2025-08-06T12:55:31.933863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import ViTForImageClassification\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision\n\n# --- 1. GAN Configuration ---\nlatent_dim = 100\nlr_gan = 0.0002\nbeta1 = 0.5\ngan_epochs = 30 \ngan_batch_size = 128\n\n\n# --- 2. Model Definitions ---\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False), nn.Tanh()\n        )\n    def forward(self, z):\n        return self.model(z)\n\n# The Discriminator using a pre-trained ViT\nclass ViTDiscriminator(nn.Module):\n    def __init__(self):\n        super(ViTDiscriminator, self).__init__()\n        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=1, ignore_mismatched_sizes=True)\n        self.vit.classifier = nn.Linear(self.vit.config.hidden_size, 1)\n\n    def forward(self, img):\n        # Before passing the image to the ViT, we resize it from 64x64 to 224x224 for pretrained vit compatibility\n        upsampled_img = F.interpolate(img, size=(224, 224), mode='bilinear', align_corners=False)\n        return self.vit(upsampled_img).logits\n\n\n# --- 3. Initialization ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n\nnetG = Generator(latent_dim)\nnetD = ViTDiscriminator()\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs for ViT-GAN!\")\n    netG = nn.DataParallel(netG)\n    netD = nn.DataParallel(netD)\n\nnetG.to(device)\nnetD.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizerD = torch.optim.Adam(netD.parameters(), lr=lr_gan, betas=(beta1, 0.999))\noptimizerG = torch.optim.Adam(netG.parameters(), lr=lr_gan, betas=(beta1, 0.999))\n\ngan_dataloader = torch.utils.data.DataLoader(subset_dataset, batch_size=gan_batch_size, shuffle=True, num_workers=2)\n\n# --- 4. GAN Training Loop ---\nprint(\"Starting ViT-GAN Training...\")\nfor epoch in range(gan_epochs):\n    progress_bar = tqdm(total=len(gan_dataloader))\n    progress_bar.set_description(f\"Epoch {epoch + 1}/{gan_epochs}\")\n    \n    for i, data in enumerate(gan_dataloader, 0):\n        ## Train Discriminator ##\n        netD.zero_grad()\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        real_label = torch.full((b_size,), 1., dtype=torch.float, device=device)\n        \n        output_real = netD(real_cpu).view(-1)\n        errD_real = criterion(output_real, real_label)\n        errD_real.backward()\n\n        noise = torch.randn(b_size, latent_dim, 1, 1, device=device)\n        fake = netG(noise)\n        fake_label = torch.full((b_size,), 0., dtype=torch.float, device=device)\n        \n        output_fake = netD(fake.detach()).view(-1)\n        errD_fake = criterion(output_fake, fake_label)\n        errD_fake.backward()\n        \n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        ## Train Generator ##\n        netG.zero_grad()\n        real_label.fill_(1.)\n        output = netD(fake).view(-1)\n        errG = criterion(output, real_label)\n        errG.backward()\n        optimizerG.step()\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix({\"Loss_D\": errD.item(), \"Loss_G\": errG.item()})\n\n    with torch.no_grad():\n        unwrapped_G = netG.module if isinstance(netG, nn.DataParallel) else netG\n        fake_samples = unwrapped_G(fixed_noise).detach().cpu()\n\n    print(f\"\\nGenerating samples after epoch {epoch+1}:\")\n    grid = torchvision.utils.make_grid(fake_samples, padding=2, normalize=True)\n    plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n    plt.title(f\"ViT-GAN Generated Images - Epoch {epoch+1}\")\n    plt.imshow(np.transpose(grid,(1,2,0)))\n    plt.show()\n\nprint(\"\\nViT-GAN training complete!\")\n\n# --- 5. Store final generated images for comparison ---\nwith torch.no_grad():\n    unwrapped_G = netG.module if isinstance(netG, nn.DataParallel) else netG\n    # Generate the same number of images as the diffusion model for a fair comparison\n    vit_gan_images_tensor = unwrapped_G(torch.randn(config.eval_batch_size, latent_dim, 1, 1, device=device)).detach().cpu()\n    \n    vit_gan_images = vit_gan_images_tensor.permute(0, 2, 3, 1).numpy()\n    vit_gan_images = (vit_gan_images * 0.5) + 0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:58:03.309694Z","iopub.execute_input":"2025-08-06T12:58:03.310473Z","iopub.status.idle":"2025-08-06T13:41:14.934049Z","shell.execute_reply.started":"2025-08-06T12:58:03.310442Z","shell.execute_reply":"2025-08-06T13:41:14.93325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Install the library for calculating FID score\n!pip install -q torch-fidelity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T13:41:52.054245Z","iopub.execute_input":"2025-08-06T13:41:52.054872Z","iopub.status.idle":"2025-08-06T13:41:55.179378Z","shell.execute_reply.started":"2025-08-06T13:41:52.054839Z","shell.execute_reply":"2025-08-06T13:41:55.178516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision\nfrom torch_fidelity import calculate_metrics\nfrom PIL import Image\nimport os\n\n# --- 1. Helper function to save images ---\ndef save_images_to_folder(images_numpy, folder_path):\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n    \n    for i, img_np in enumerate(images_numpy):\n        img_uint8 = (img_np * 255).astype(np.uint8)\n        img = Image.fromarray(img_uint8)\n        img.save(os.path.join(folder_path, f\"image_{i}.png\"))\n\n\n\n# --- 2. Qualitative (Visual) Comparison ---\n\nprint(\"--- VISUAL COMPARISON ---\")\n\n# Display Diffusion Model Images\nprint(\"\\nImages from Diffusion Model:\")\ndiffusion_grid = torchvision.utils.make_grid(torch.from_numpy(diffusion_images).permute(0, 3, 1, 2))\nnp_diffusion_grid = diffusion_grid.numpy()\nplt.figure(figsize=(8, 8))\nplt.imshow(np.transpose(np_diffusion_grid, (1, 2, 0)))\nplt.axis(\"off\")\nplt.title(\"Diffusion Model Generated Images\")\nplt.show()\n\n\n# Display ViT-GAN Images\nprint(\"\\nImages from ViT-GAN Model:\")\n\n# We convert the numpy array (B, H, W, C) to a torch tensor and\n# permute it to (B, C, H, W) which is the format `make_grid` expects.\nvit_gan_tensor_for_grid = torch.from_numpy(vit_gan_images).permute(0, 3, 1, 2)\nvit_gan_grid = torchvision.utils.make_grid(vit_gan_tensor_for_grid)\nnp_vit_gan_grid = vit_gan_grid.numpy()\nplt.figure(figsize=(8, 8))\n# Now we need to transpose it back for matplotlib, just like the diffusion grid\nplt.imshow(np.transpose(np_vit_gan_grid, (1, 2, 0)))\nplt.axis(\"off\")\nplt.title(\"ViT-GAN Generated Images\")\nplt.show()\n\n\n# --- 3. Quantitative (FID Score) Comparison ---\n\nprint(\"\\n--- QUANTITATIVE COMPARISON (FID SCORE) ---\")\nprint(\"A lower FID score is better.\")\n\nREAL_IMAGES_DIR = './data/cifar10_real_for_fid'\nDIFFUSION_IMAGES_DIR = './gen_images_diffusion'\nVIT_GAN_IMAGES_DIR = './gen_images_vit_gan'\n\nsave_images_to_folder(diffusion_images, DIFFUSION_IMAGES_DIR)\nsave_images_to_folder(vit_gan_images, VIT_GAN_IMAGES_DIR)\n\nprint(\"\\nPreparing a folder of real images for comparison...\")\nif not os.path.exists(REAL_IMAGES_DIR):\n    real_images_for_fid = []\n    for i, (img, _) in enumerate(dataset):\n        if i >= 1000:\n            break\n        img_np = img.permute(1, 2, 0).numpy() * 0.5 + 0.5\n        real_images_for_fid.append(img_np)\n    save_images_to_folder(np.array(real_images_for_fid), REAL_IMAGES_DIR)\nelse:\n    print(\"Real images folder already exists.\")\n\n\nprint(\"\\nCalculating FID for Diffusion Model... (this may take a minute)\")\nmetrics_dict_diffusion = calculate_metrics(\n    input1=DIFFUSION_IMAGES_DIR, \n    input2=REAL_IMAGES_DIR, \n    cuda=True, \n    fid=True,\n    verbose=False\n)\nprint(f\"Diffusion Model FID: {metrics_dict_diffusion['frechet_inception_distance']:.2f}\")\n\nprint(\"\\nCalculating FID for ViT-GAN Model...\")\nmetrics_dict_vit_gan = calculate_metrics(\n    input1=VIT_GAN_IMAGES_DIR, \n    input2=REAL_IMAGES_DIR, \n    cuda=True, \n    fid=True,\n    verbose=False\n)\nprint(f\"ViT-GAN FID: {metrics_dict_vit_gan['frechet_inception_distance']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T13:43:53.162573Z","iopub.execute_input":"2025-08-06T13:43:53.163239Z","iopub.status.idle":"2025-08-06T13:44:20.307576Z","shell.execute_reply.started":"2025-08-06T13:43:53.163213Z","shell.execute_reply":"2025-08-06T13:44:20.306796Z"}},"outputs":[],"execution_count":null}]}